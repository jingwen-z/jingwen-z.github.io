---
layout:             post
title:              UCSD Introduction to Big Data Week 1 & 2 review
date:               2016-12-07 23:32:43 +0200
tags:               [MOOC]
comments:           true
article_header:
  type: overlay
  theme: dark
  background_color: "#203028"
  background_image:
    gradient: "linear-gradient(135deg, rgba(0, 0, 0, .6), rgba(0, 0, 0, .4))"
---

Since big data becomes more and more important in our life. As a fresh graduate
in Economics and Statistics, I'm eager to learn more knowledge about big data.
Thus, I decide to participate the course [Big Data specilization][Big Data
specilization], created by University of California, San Diego, taught by _Ilkay
Altintas_ (Chief Data Science Officer), _Amarnath Gupta_ (Director, Advanced
Query Processing Lab) and _Mai Nguyen_ (Lead for Data Analytics), they all work
in San Diego Supercomputer Center(SDSC).

This specilization contains 6 courses as follows:

- [Introduction to Big Data][Introduction]
- [Big Data Modeling and Management Systems][Management]
- [Big Data Integration and Processing][Integration and Processing]
- [Machine Learning With Big Data][Machine Learning]
- [Graph Analytics for Big Data][Graph Analytics]
- [Big Data - Capstone Project][project]

In this blog, I'll share what I learnt about the first two courses, Introduction
 to Big Data, during the first week.

Before learning Big Data technique, let's talk about the sources of Big Data.

## Where does Big Data come from?

Big Data mainly comes from three sources: **machine**, **people** and
**organization**. In the following, I'll talk about them one by one.

### Machine

Machine data is the largest source of big data, which presents the notion
_Internet of Thing_(IoT). Think of a world of smart devices at home, in your car,
 in the office, city, remote rural areas, the sky, even the ocean, all connected
  and all generating data.

### People

People are generating massive amounts of data everyday through their activites
on various social media networking sites like Facebook, Twitter and LinkedIn,
online photo sharing sites like Instagram.

Most of these data are text-heavy and unstructured, which bring challenges of
working. This is certainly the case for big data and these challenges have
created a tech industry of its own. Many big data tools are designed from
scratch to manage unstructured information and analyze it, like _Hadoop, Spark_
etc. I'll talk about them later.

### Organization

The last source of big data we will discuss is organization. How do
organizations producd data? Each organization has distinct operation practices
and business models, which result in a variety of data generation platforms.

Many organizations have traditionally captured data at the department level,
without proper infrastructure and policy to share and integrate this data. This
has hindered the growth of scalable pattern recognition to the benefits of the
entire organization. Because no one system has access to all data that the
organization owns.

Organizations are realizing the detrimental outcomes of this rigid structure,
and changing policies and infrastructure to enable integrated processing of all
data to the entire organization's benefit.

While, how are organizations benefiting from big data? Let's take an example of
Walmart. They collect data on Twitter tweets, local events, local weather,
in-store purchases, online clicks and many other sales, customer and product
related data. They use this data to find patterns such as which products are
frequently purchased together, and what is the best new product to introduce in
their stores, to predict demand at the particular location, and to customize
customer recommendations. Overall, by leveraging big data and analytics, Walmart
 has maintained its position as a top retailer.

As a summary, organizations are gaining significant benefit from integrating big
 data practices into their culture and breaking their silos. Some major benefits
  to organizations are _operational efficiency, improved marketing outcomes,
  higher profits,_ and _improved customer satisfaction_.

## Characteristics of Big Data (V's)

Big data is commonly characterized using a number of V's.

### Volume

This refers to the _vast amounts_ of data that is generated every second/minute/
hour/day in our digitized world.

In general, in business the goal is to turn this much data into some form of
business advantage. The question is how do we utilize larger volumes of data to
improve our end product's quality? Despite a number of challenges related to it.

The most obvious challenge is storage. As the size of the data increases so does
 the amount of storage space required to store that data efficiently. However,
 we also need to be able to retrieve that large amount of data fast enough, and
 move it to processing units in a timely fashion to get results when we need
them. This means their performance will drop.

As a summary, the challenges with working with volumes of big data include _cost,
 scalability, and performance_ related to their storage, access, and processing.

### Velocity

This refers to the _speed_ at which data is being generated and the pace at
which data moves from one point to the next. This brings additional challenges
such as networking, bandwidth, cost of storing data. In-house versus cloud
storage and things like that.

Additional challenges arise during processing of such large data. Most existing
analytical methods won't scale to such sums of data in terms of memory,
processing, or IO needs.

### Variety

This refers to the ever-increasing _different forms_ that data can come in, e.g.
text, images, voice, geospatial.

The heterogeneity of data can be characterized along several dimensions. We
mentioned four such axes here.

- **Structural variety** refers to the difference in the representation of the
data, like formats and models. For example, an EKG signal is very different from
 a newspaper article.

- **Media variety** refers to the medium in which the data gets delivered. The
audio of a speech verses the transcript of the speech may represent the same
information in two different media.

- **Semantic variety** refers to the method of interpretation and operation on
data. We often use different units for quantities we measure. Sometimes we also
use qualitative versus quantitative measures. For example, age can be a number
or we represent it by terms like infant, juvenile, or adult. Another kind of
semantic variety comes from different assumptions of conditions on the data. For
 example, if we conduct two income surveys on two different groups of people, we
 may not be able to compare or combine them without knowing more about the
populations themselves.

- The **variation** and **availability** takes many forms. For one, data can be
available real time, like sensor data, or it can be stored, like patient records.
Similarly data can be accessible continuously, for example from a traffic cam.
Versus intermittently, for example, only when the satellite is over the region
of interest. This makes a difference between what operations one can do with
data, especially if the volume of the data is large.

Thus, data variety has many impacts like _be harder to ingest, be difficult to
create common storage, be difficult to compare and match data across variety, be
difficult to integrate_ and _management and policy challenges_ as well.

### Veracity

This refers to the _quality_ of the data, which can vary greatly. Because big
data can be noisy and uncertain. It can be full of biases, abnormalities and it
can be imprecise. Data is of no value if it's not accurate, the results of big
data analysis are only as good as the data being analyzed. So we can say
although big data provides many opportunities to make data enabled decisions,
the evidence provided by data is only valuable if the data is of a satisfactory
quality.

There are many different ways to define data quality. In the context of big data,
 quality can be defined as a function of a couple of different variables.
**Accuracy** of the data, the trustworthiness or reliability of the data source.
And **how the data was generated** are all important factors that affect the
quality of data. Additionally **how meaningful the data is** with respect to the
program that analyzes it, is an important factor, and makes context a part of
the quality.

This creates challenges on keeping track of data quality. _What has been
collected, where it came from,_ and _how it was analyzed prior to its use_.

### Valence

This refers to _how big data can bond with each other_, forming connections
between otherwise disparate datasets. For a data collection valence measures the
 ratio of actually connected data items to the possible number of connections
that could occur within the collection. The most important aspect of valence is
that the data connectivity increases over time.

Thus, valence brings some challenges. _A high valence data set is denser_. This
makes many regular, analytic critiques very inefficient. More complex analytical
 methods must be adopted to account for the increasing density. More interesting
 challenges arise due to the dynamic behavior of the data. Now there is a need
to _model and predict_ how valence of a connected data set may change with time
and volume. The dynamic behavior also leads to the problem of _event detection_,
such as bursts in the local cohesion in parts of the data. And _emergent
behavior_ in the whole data set, such as increased polarization in a community.


Voil√†, here are what I want to share with you. In the [review of week 3][review of week 3],
I will talk about the process of data analysis and Hadoop.

[Big Data specilization]: https://www.coursera.org/specializations/big-data
[Introduction]: https://www.coursera.org/learn/big-data-introduction
[Management]: https://www.coursera.org/learn/big-data-management
[Integration and Processing]: https://www.coursera.org/learn/big-data-integration-processing
[Machine Learning]: https://www.coursera.org/learn/big-data-machine-learning
[Graph Analytics]: https://www.coursera.org/learn/big-data-graph-analytics
[project]: https://www.coursera.org/learn/big-data-project
[review of week 3]: https://jingwen-z.github.io/introduction-bd-week3
